{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550ad25a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a24625",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/OptiML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install foolbox==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc4ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from Functions.implementations import *\n",
    "from Functions.visualization import *\n",
    "import foolbox as fb\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cf5b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Starting Black Box Attacks...\n",
      "Running black box attack on VGG with SGD optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Boundary Attack:   0%|          | 0/4 [02:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis -1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(csv_path))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Run the attack and save the results\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m results \u001b[38;5;241m=\u001b[39m  attack_model(model, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([optimizer] \u001b[38;5;241m+\u001b[39m [model_name] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(results))\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\samue\\Desktop\\Box Sync\\3.Samuel\\Uni_24-25\\Opt_ML\\OptML-project\\Functions\\implementations.py:483\u001b[0m, in \u001b[0;36mattack_model\u001b[1;34m(model, num_images)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     robust_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m success\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# Calculate robust accuracy which is the accuracy of the model when it is attacked\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     perturbation_sizes\u001b[38;5;241m.\u001b[39mappend((clipped_advs \u001b[38;5;241m-\u001b[39m image)\u001b[38;5;241m.\u001b[39mnorm()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mmean(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(clean_accuracy), np\u001b[38;5;241m.\u001b[39mmean(robust_accuracy), np\u001b[38;5;241m.\u001b[39mmean(perturbation_sizes)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\numpy\\_core\\_methods.py:123\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    119\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m    121\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    125\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\numpy\\_core\\_methods.py:86\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     84\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[1;32m---> 86\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[mu\u001b[38;5;241m.\u001b[39mnormalize_axis_index(ax, arr\u001b[38;5;241m.\u001b[39mndim)]\n\u001b[0;32m     87\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis -1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "set_seed(42) # Set seed through custom function as done throughtout the project\n",
    "\n",
    "best_models_dir = os.path.join(os.getcwd(), os.path.join(\"Results\",\"Best_models\"))\n",
    "\n",
    "print(\"Loading models...\")\n",
    "# Load the best performing models\n",
    "SGD_best_models = [\"SGD_VGG_Transform_lr_0.001_momentum_0.99.pth\", \"SGD_ResNet_Transform_lr_0.05_momentum_0.9.pth\", \"SGD_DenseNet_Transform_lr_0.01_momentum_0.99.pth\"]\n",
    "Adam_best_models = [\"ADAM_VGG_lr_0.0005_beta1_0.9_beta2_0.98.pth\", 'ADAM_ResNet_lr_0.001_beta1_0.8_beta2_0.999.pth', 'ADAM_DenseNet_lr_0.001_beta1_0.8_beta2_0.9999.pth']\n",
    "Adagrad_best_models = []\n",
    "\n",
    "sgd_models = get_best_models(SGD_best_models, best_models_dir)\n",
    "adam_models = get_best_models(Adam_best_models, best_models_dir)\n",
    "adagrad_models = get_best_models(Adagrad_best_models, best_models_dir)\n",
    "    \n",
    "set_seed(42)  # Reset seed to ensure reproducibility after loading models\n",
    "\n",
    "list_of_optimizer_dicts = [sgd_models, adam_models, adagrad_models] # Combine the models into a list of dictionaries for easier iteration\n",
    "\n",
    "print(\"Starting Black Box Attacks...\")\n",
    "    \n",
    "csv_path = os.path.join(os.path.join(best_models_dir, \"BlackBoxAttack.csv\"))\n",
    "\n",
    "for i in range(3):\n",
    "    optimizer = [\"SGD\", \"Adam\", \"Adagrad\"][i]\n",
    "    model_dict = list_of_optimizer_dicts[i]\n",
    "    for model_name, model in model_dict.items():\n",
    "        print(f\"Running black box attack on {model_name} with {optimizer} optimizer...\")\n",
    "        # Define the path to save the results\n",
    "        if not os.path.exists(os.path.dirname(csv_path)):\n",
    "            columns = [\"optimizer\", \"model\", \"success_rate\",\"avg_perturbation_size\"]\n",
    "            pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "            os.makedirs(os.path.dirname(csv_path))\n",
    "            \n",
    "        # Run the attack and save the results\n",
    "        results =  attack_model(model, 4)\n",
    "        # Save the results to a CSV file\n",
    "        df = pd.DataFrame([optimizer] + [model_name] + list(results)).T\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317dc943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load one image from CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "image, label = dataset[0]\n",
    "image = image.unsqueeze(0)  # Add batch dimension\n",
    "label = torch.tensor([label])\n",
    "\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c89f00d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels= []\n",
    "for i in range(16):\n",
    "    images.append(dataset[i][0].unsqueeze(0))  # Add batch dimension\n",
    "    labels.append(torch.tensor([dataset[i][1]]))\n",
    "\n",
    "for image in images:\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e75448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_n_batches(model, test_loader, device, n_batches=3, attack = fb.attacks.BoundaryAttack()):\n",
    "    \"\"\"\n",
    "    Run a black-box attack on a model for a specified number of batches.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to attack.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        n_batches: Number of batches to process.\n",
    "        \n",
    "    Returns:\n",
    "        success_rate: Success rate of the attack.\n",
    "        avg_perturbation: Average perturbation of the adversarial examples.\n",
    "        avg_queries: Average number of queries made (if applicable).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    fmodel = fb.PyTorchModel(model, bounds=(0, 1))\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    successes, perturbations, queries = [], [], []\n",
    "\n",
    "    # Iterate over three batches of images\n",
    "    for i in tqdm(range(n_batches), desc=\"Attacking model\"):\n",
    "        images, labels = next(iter(test_loader))\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Optionally, filter out misclassified samples\n",
    "        preds = model(images).argmax(dim=1)\n",
    "        mask = preds == labels\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        images, labels = images[mask], labels[mask]\n",
    "        # Run attack\n",
    "        raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=None)\n",
    "        # Collect metrics\n",
    "        successes.append(success.cpu())\n",
    "        perturbations.append((clipped_advs - images).view(images.size(0), -1).norm(dim=1).cpu())\n",
    "        # If available, track queries (depends on attack implementation)\n",
    "        queries.append(attack.queries.cpu()) if hasattr(attack, 'queries') else None\n",
    "    # Aggregate and report\n",
    "    if successes:\n",
    "        success_rate = torch.cat(successes).float().mean().item()\n",
    "        avg_perturbation = torch.cat(perturbations).mean().item()\n",
    "        avg_queries = torch.cat(queries).float().mean().item() if queries else None\n",
    "    else:\n",
    "        print(\"No successful batches processed. Check your data or model accuracy.\")\n",
    "        success_rate, avg_perturbation, avg_queries = None, None, None\n",
    "    return success_rate, avg_perturbation, avg_queries\n",
    "\n",
    "# extract the learning rate, beta_1, and beta_2 from the model name\n",
    "def extract_params(model_name):\n",
    "    parts = model_name.split('_')\n",
    "    learning_rate = float(parts[2])\n",
    "    beta_1 = float(parts[4])\n",
    "    beta_2 = float(parts[6][:-4])\n",
    "    return learning_rate, beta_1, beta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42) # Set seed through custom function as done throughtout the project\n",
    "\n",
    "n_batches = 16\n",
    "batch_size = 16\n",
    "_, _, test_loader = get_data_loaders(batch_size=batch_size)\n",
    "\n",
    "drive_base_path = os.getcwd()\n",
    "folder_path = os.path.join(drive_base_path, 'Results/Adam_densenet')\n",
    "csv_path = os.path.join(folder_path, 'densenetBlackBox_topmodel.csv')\n",
    "\n",
    "#model_list = ['VGG_lr_0.0005_beta1_0.9_beta2_0.98.pth']#, 'VGG_lr_0.0005_beta1_0.8_beta2_0.99.pth', 'VGG_lr_0.0005_beta1_0.8_beta2_0.98.pth'] # for VGG models\n",
    "#model_list = ['resnet_lr_0.001_beta1_0.8_beta2_0.999.pth']#, 'resnet_lr_0.001_beta1_0.9_beta2_0.99.pth', 'resnet_lr_0.001_beta1_0.8_beta2_0.9999.pth'] # for resnet models\n",
    "model_list = ['densenet_lr_0.001_beta1_0.8_beta2_0.9999.pth']#, 'densenet_lr_0.0005_beta1_0.9_beta2_0.999.pth', 'densenet_lr_0.001_beta1_0.9_beta2_0.9999.pth'] # for densenet models\n",
    "\n",
    "attack_list = [\n",
    "    fb.attacks.BoundaryAttack(),\n",
    "    fb.attacks.GaussianBlurAttack(),\n",
    "    fb.attacks.SaltAndPepperNoiseAttack()\n",
    "]\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    columns = [\"file_name\", \"learning_rate\", \"beta_1\", \"beta_2\", \"attack_type\",\"success_rate\",\"avg_perturbations\",\"avg_queries\"]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "for model_name in model_list:\n",
    "    if os.path.exists(os.path.join(folder_path, model_name)):\n",
    "        print(f\"File {model_name} exists in the folder.\")\n",
    "    else:\n",
    "        print(f\"File {model_name} does not exist in the folder. Please check the path or file name.\")\n",
    "        continue\n",
    "    print(\"Loading model: \", model_name)\n",
    "    if not model_name.endswith('.pth'):\n",
    "        continue\n",
    "    model_path = os.path.join(folder_path, model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = VGGLike().to(device) # for VGG models\n",
    "    #model = get_resnet18_cifar().to(device) # for resnet models\n",
    "    model = get_densenet121().to(device) # for densenet models\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    for attack in attack_list:\n",
    "        print(f\"Running attack: {attack.__class__.__name__}\")\n",
    "        results = attack_n_batches(model, test_loader, device, n_batches, attack)\n",
    "        print(\"Saving results\")\n",
    "        df = pd.DataFrame([model_name] + list(extract_params(model_name)) + [attack.__class__.__name__] + list(results)).T\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "        clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b2fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
