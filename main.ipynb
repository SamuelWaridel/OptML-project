{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook for running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from Functions.implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "train_loader,valid_loader, test_loader = get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a simple model to test if everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, train_loader, test_loader, optimizer, criterion, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#only for google Colab :\n",
    "\n",
    "# Set a path in your Google Drive\n",
    "#csv_path = \"/content/drive/MyDrive/OptiML/sgd_gridsearch_results.csv\"\n",
    "\n",
    "# For local machine, set the path to your desired location\n",
    "save_path = os.getcwd() + \"/Results/SGD\"\n",
    "csv_path = save_path + \"/sgd_gridsearch_results.csv\"\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "eval_interval = 10\n",
    "\n",
    "# Create the CSV with headers if it doesn't exist\n",
    "if not os.path.exists(csv_path):\n",
    "    columns = [\"learning_rate\", \"momentum\"] +  [f\"epoch_{i}\" for i in range(epochs//eval_interval, epochs + 1, eval_interval)] + [\"Test\"]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "    \n",
    "set_seed(42) # Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training with SGD: lr=0.01, momentum=0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate over all combinations of lr and momentum\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr, momentum \u001b[38;5;129;01min\u001b[39;00m product(learning_rates, momentums):\n\u001b[1;32m---> 17\u001b[0m     scores, model \u001b[38;5;241m=\u001b[39m train_and_return_evaluation_SGD(\n\u001b[0;32m     18\u001b[0m         SimpleCNN,\n\u001b[0;32m     19\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     20\u001b[0m         momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[0;32m     21\u001b[0m         train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     22\u001b[0m         valid_loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[0;32m     23\u001b[0m         test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m     24\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     25\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     26\u001b[0m         eval_interval\u001b[38;5;241m=\u001b[39meval_interval\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     results_grid[(lr, momentum)] \u001b[38;5;241m=\u001b[39m scores\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Extract F1 scores only, for CSV export\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samue\\Desktop\\Box Sync\\3.Samuel\\Uni_24-25\\Opt_ML\\OptML-project\\Functions\\implementations.py:214\u001b[0m, in \u001b[0;36mtrain_and_return_evaluation_SGD\u001b[1;34m(model_fn, lr, momentum, train_loader, valid_loader, test_loader, device, epochs, eval_interval)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”§ Training with SGD: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, momentum=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmomentum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 214\u001b[0m     train_one_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    216\u001b[0m         acc, rec, f1 \u001b[38;5;241m=\u001b[39m evaluate_model(model, valid_loader, device)\n",
      "File \u001b[1;32mc:\\Users\\samue\\Desktop\\Box Sync\\3.Samuel\\Uni_24-25\\Opt_ML\\OptML-project\\Functions\\implementations.py:165\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Train the model for one epoch.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mThe model is set to training mode.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03mThe optimizer and loss function are used to update the model weights.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mThe loss is calculated and backpropagated.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    164\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    166\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    168\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader,valid_loader, test_loader = get_data_loaders(batch_size=128)\n",
    "\n",
    "results_grid = {}\n",
    "\n",
    "# learning_rates = [5e-2, 1e-2, 5e-3, 1e-3]\n",
    "# momentums = [0.0, 0.6, 0.9, 0.99]\n",
    "\n",
    "learning_rates = [1e-2, 5e-3]\n",
    "momentums = [0.9]\n",
    "\n",
    "# Iterate over all combinations of lr and momentum\n",
    "for lr, momentum in product(learning_rates, momentums):\n",
    "    scores, model = train_and_return_evaluation_SGD(\n",
    "        SimpleCNN,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "    results_grid[(lr, momentum)] = scores\n",
    "\n",
    "    # Extract F1 scores only, for CSV export\n",
    "    f1_scores = [f1 for (_, _, _, f1) in scores]\n",
    "\n",
    "    row = [lr, momentum] + f1_scores\n",
    "\n",
    "    # Append to CSV\n",
    "    df_row = pd.DataFrame([row])\n",
    "    df_row.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), save_path + f\"/model_lr_{lr}_momentum_{momentum}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#only for google Colab :\n",
    "\n",
    "# Set a path in your Google Drive\n",
    "#csv_path = \"/content/drive/MyDrive/OptiML/sgd_gridsearch_results.csv\"\n",
    "\n",
    "# For local machine, set the path to your desired location\n",
    "save_path = os.getcwd() + \"/Results/Adam\"\n",
    "csv_path = save_path + \"/adam_gridsearch_results.csv\"\n",
    "\n",
    "# Set number of epochs and evaluation interval\n",
    "epochs = 100\n",
    "eval_interval = 10\n",
    "\n",
    "# Create the CSV with headers if it doesn't exist\n",
    "if not os.path.exists(csv_path):\n",
    "    columns = [\"learning_rate\", \"momentum\"] +  [f\"epoch_{i}\" for i in range(epochs//eval_interval, epochs + 1, eval_interval)] + [\"Test\"]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "set_seed(42) # Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training with Adam: lr=0.005, betas=(0.8, 0.99)\n",
      "Epoch 10 | Acc=0.7752 | Recall=0.7752 | F1=0.7774\n",
      "Epoch 20 | Acc=0.9151 | Recall=0.9150 | F1=0.9145\n",
      "Epoch 30 | Acc=0.9540 | Recall=0.9540 | F1=0.9541\n",
      "Epoch 40 | Acc=0.9472 | Recall=0.9472 | F1=0.9471\n",
      "Epoch 50 | Acc=0.9643 | Recall=0.9643 | F1=0.9643\n",
      "Epoch 60 | Acc=0.9736 | Recall=0.9736 | F1=0.9736\n",
      "Epoch 70 | Acc=0.9700 | Recall=0.9701 | F1=0.9700\n",
      "Epoch 80 | Acc=0.9646 | Recall=0.9646 | F1=0.9645\n",
      "Epoch 90 | Acc=0.9782 | Recall=0.9782 | F1=0.9782\n",
      "Epoch 100 | Acc=0.9783 | Recall=0.9783 | F1=0.9783\n",
      "\n",
      "Test Set Evaluation: Acc=0.6104 | Recall=0.6104 | F1=0.6105\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, valid_loader, test_loader = get_data_loaders(batch_size=128)\n",
    "\n",
    "results_grid = {}\n",
    "\n",
    "# learning_rates = [5e-2, 1e-2, 5e-3, 1e-3]\n",
    "# betas = [(0.9, 0.999), (0.8, 0.999), (0.9, 0.99)]\n",
    "\n",
    "learning_rates = [5e-3]\n",
    "betas = [(0.8, 0.99)]\n",
    "\n",
    "# Iterate over all combinations of lr and momentum\n",
    "for lr, betas in product(learning_rates, betas):\n",
    "    scores, model = train_and_return_evaluation_Adam(\n",
    "        SimpleCNN,\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "    results_grid[(lr, betas[0], betas[1])] = scores\n",
    "\n",
    "    # Extract F1 scores only, for CSV export\n",
    "    f1_scores = [f1 for (_, _, _, f1) in scores]\n",
    "\n",
    "    row = [lr, betas[0], betas[1]] + f1_scores\n",
    "\n",
    "    # Append to CSV\n",
    "    df_row = pd.DataFrame([row])\n",
    "    df_row.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), save_path + f\"/model_lr_{lr}_beta1_{betas[0]}_beta2_{betas[1]}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on colab :\n",
    "\n",
    "lr = 0.05\n",
    "- (0.9, 0.999) : F1 ~ 0.18\n",
    "- (0.8, 0.999) : F1 ~ 0.18\n",
    "- (0.9, 0.99) : F1 ~ \n",
    "\n",
    "lr = 0.01\n",
    "- (0.9, 0.999) : F1 ~ \n",
    "- (0.8, 0.999) : F1 ~ \n",
    "- (0.9, 0.99) : F1 ~ \n",
    "\n",
    "lr = 0.005\n",
    "- (0.9, 0.999) : F1 ~ \n",
    "- (0.8, 0.999) : F1 ~ \n",
    "- (0.9, 0.99) : F1 ~ \n",
    "\n",
    "lr = 0.001\n",
    "- (0.9, 0.999) : F1 ~ \n",
    "- (0.8, 0.999) : F1 ~ \n",
    "- (0.9, 0.99) : F1 ~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
