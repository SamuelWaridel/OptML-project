{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook for running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from Functions.implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "train_loader, test_loader = get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a simple model to test if everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, train_loader, test_loader, optimizer, criterion, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#only for google Colab :\n",
    "\n",
    "# Set a path in your Google Drive\n",
    "#csv_path = \"/content/drive/MyDrive/OptiML/sgd_gridsearch_results.csv\"\n",
    "\n",
    "# For local machine, set the path to your desired location\n",
    "csv_path = os.getcwd() + \"/Results/SGD/sgd_gridsearch_results.csv\"\n",
    "\n",
    "# Create the CSV with headers if it doesn't exist\n",
    "if not os.path.exists(csv_path):\n",
    "    columns = [\"learning_rate\", \"momentum\"] + [f\"epoch_{i}\" for i in range(10, 101, 10)]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "eval_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42) # Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_data_loaders(batch_size=128)\n",
    "\n",
    "results_grid = {}\n",
    "\n",
    "# learning_rates = [5e-2, 1e-2, 5e-3, 1e-3]\n",
    "# momentums = [0.0, 0.6, 0.9, 0.99]\n",
    "\n",
    "learning_rates = [1e-2]\n",
    "momentums = [0.0, 0.6, 0.9]\n",
    "\n",
    "# Iterate over all combinations of lr and momentum\n",
    "for lr, momentum in product(learning_rates, momentums):\n",
    "    scores = train_and_return_evaluation_SGD(\n",
    "        VGGLike,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "    results_grid[(lr, momentum)] = scores\n",
    "\n",
    "    # Extract F1 scores only, for CSV export\n",
    "    f1_scores = [f1 for (_, _, _, f1) in scores]\n",
    "\n",
    "    row = [lr, momentum] + f1_scores\n",
    "\n",
    "    # Append to CSV\n",
    "    df_row = pd.DataFrame([row])\n",
    "    df_row.to_csv(csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#only for google Colab :\n",
    "\n",
    "# Set a path in your Google Drive\n",
    "#csv_path = \"/content/drive/MyDrive/OptiML/sgd_gridsearch_results.csv\"\n",
    "\n",
    "# For local machine, set the path to your desired location\n",
    "csv_path = os.getcwd() + \"/Results/Adam/adam_gridsearch_results.csv\"\n",
    "\n",
    "# Create the CSV with headers if it doesn't exist\n",
    "if not os.path.exists(csv_path):\n",
    "    columns = [\"learning_rate\", \"decay_rate_1\", \"decay_rate_2\"] + [f\"epoch_{i}\" for i in range(10, 101, 10)]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "# Set number of epochs and evaluation interval\n",
    "epochs = 100\n",
    "eval_interval = 10\n",
    "\n",
    "set_seed(42) # Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training with Adam: lr=0.01, betas=(0.9, 0.999)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate over all combinations of lr and momentum\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr, betas \u001b[38;5;129;01min\u001b[39;00m product(learning_rates, betas):\n\u001b[1;32m---> 17\u001b[0m     scores \u001b[38;5;241m=\u001b[39m train_and_return_evaluation_Adam(\n\u001b[0;32m     18\u001b[0m         VGGLike,\n\u001b[0;32m     19\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     20\u001b[0m         betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m     21\u001b[0m         train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     22\u001b[0m         test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m     23\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     24\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     25\u001b[0m         eval_interval\u001b[38;5;241m=\u001b[39meval_interval\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     results_grid[(lr, betas[\u001b[38;5;241m0\u001b[39m], betas[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;241m=\u001b[39m scores\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Extract F1 scores only, for CSV export\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samue\\Desktop\\Box Sync\\3.Samuel\\Uni_24-25\\Opt_ML\\OptML-project\\Functions\\implementations.py:240\u001b[0m, in \u001b[0;36mtrain_and_return_evaluation_Adam\u001b[1;34m(model_fn, lr, betas, train_loader, test_loader, device, epochs, eval_interval)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”§ Training with Adam: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, betas=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbetas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 240\u001b[0m     train_one_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    242\u001b[0m         acc, rec, f1 \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n",
      "File \u001b[1;32mc:\\Users\\samue\\Desktop\\Box Sync\\3.Samuel\\Uni_24-25\\Opt_ML\\OptML-project\\Functions\\implementations.py:150\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    148\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    149\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 150\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    151\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\optml\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_data_loaders(batch_size=128)\n",
    "\n",
    "results_grid = {}\n",
    "\n",
    "# learning_rates = [5e-2, 1e-2, 5e-3, 1e-3]\n",
    "# betas = [(0.9, 0.999), (0.8, 0.999), (0.9, 0.99)]\n",
    "\n",
    "learning_rates = [1e-2]\n",
    "betas = [(0.9, 0.999), (0.8, 0.999), (0.9, 0.99)]\n",
    "\n",
    "# Iterate over all combinations of lr and momentum\n",
    "for lr, betas in product(learning_rates, betas):\n",
    "    scores = train_and_return_evaluation_Adam(\n",
    "        VGGLike,\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "    results_grid[(lr, betas[0], betas[1])] = scores\n",
    "\n",
    "    # Extract F1 scores only, for CSV export\n",
    "    f1_scores = [f1 for (_, _, _, f1) in scores]\n",
    "\n",
    "    row = [lr, betas[0], betas[1]] + f1_scores\n",
    "\n",
    "    # Append to CSV\n",
    "    df_row = pd.DataFrame([row])\n",
    "    df_row.to_csv(csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
